# import streamlit as st
# import os
# import time
# from pathlib import Path
# import wave
# import pyaudio
# import subprocess
# import numpy as np
# from scipy.io.wavfile import write
# from langchain.prompts import (
#     ChatPromptTemplate,
#     HumanMessagePromptTemplate,
#     MessagesPlaceholder,
# )
# from langchain.schema import SystemMessage
# from langchain.memory import ConversationSummaryBufferMemory
# from langchain_openai import ChatOpenAI
# from langchain.chains import ConversationChain
# import constants as ct
import streamlit as st
import os
import time
import io
from pathlib import Path
import subprocess

from pydub import AudioSegment, silence
from streamlit_webrtc import webrtc_streamer, WebRtcMode

from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain.schema import SystemMessage
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain

import constants as ct


def record_audio(audio_input_file_path):
    """
    ğŸ¤ Streamlitæ¨™æº–ã®st.audio_inputã‚’ä½¿ç”¨ã—ã¦éŸ³å£°ã‚’éŒ²éŸ³ãƒ»ä¿å­˜ã™ã‚‹é–¢æ•°
    Args:
        audio_input_file_path: ä¿å­˜å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """

    st.info("ä¸‹ã®ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦è©±ã—ã¦ãã ã•ã„ã€‚éŒ²éŸ³å¾Œã€è‡ªå‹•ã§ä¿å­˜ã•ã‚Œã¾ã™ã€‚")

    # Streamlitæ¨™æº–ã®éŸ³å£°å…¥åŠ›ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
    audio_bytes = st.audio_input("ğŸ™ï¸ éŸ³å£°ã‚’éŒ²éŸ³ã—ã¦ãã ã•ã„")

    # éŒ²éŸ³ã•ã‚ŒãŸå ´åˆã®ã¿ä¿å­˜
    if audio_bytes:
        # âœ… UploadedFile ãªã®ã§ read() ã§ bytes ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹
        audio_data = audio_bytes.read()

        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒã‚¤ãƒŠãƒªã§æ›¸ãè¾¼ã‚€
        with open(audio_input_file_path, "wb") as f:
            f.write(audio_data)

        st.success("âœ… éŸ³å£°ãŒä¿å­˜ã•ã‚Œã¾ã—ãŸï¼")
    else:
        st.stop()



# def record_audio(audio_input_file_path):
#     """
#     ğŸ¤ Streamlitæ¨™æº–ã®st.audio_inputã‚’ä½¿ç”¨ã—ã¦éŸ³å£°ã‚’éŒ²éŸ³ãƒ»ä¿å­˜ã™ã‚‹é–¢æ•°
#     Args:
#         audio_input_file_path: ä¿å­˜å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
#     """

#     st.info("ä¸‹ã®ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦è©±ã—ã¦ãã ã•ã„ã€‚éŒ²éŸ³å¾Œã€è‡ªå‹•ã§ä¿å­˜ã•ã‚Œã¾ã™ã€‚")

#     # Streamlitæ¨™æº–ã®éŸ³å£°å…¥åŠ›ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
#     audio_bytes = st.audio_input("ğŸ™ï¸ éŸ³å£°ã‚’éŒ²éŸ³ã—ã¦ãã ã•ã„")

#     # éŒ²éŸ³ã•ã‚ŒãŸå ´åˆã®ã¿ä¿å­˜
#     if audio_bytes:
#         with open(audio_input_file_path, "wb") as f:
#             f.write(audio_bytes)
#         st.success("âœ… éŸ³å£°ãŒä¿å­˜ã•ã‚Œã¾ã—ãŸï¼")
#     else:
#         st.stop()

def transcribe_audio(audio_input_file_path):
    """
    æ—¢å­˜ãƒ¢ãƒ¼ãƒ‰ç”¨ï¼šéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ–‡å­—èµ·ã“ã—ï¼ˆãã®å¾Œãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼‰
    """
    with open(audio_input_file_path, "rb") as audio_input_file:
        transcript = st.session_state.openai_obj.audio.transcriptions.create(
            model="whisper-1",
            file=audio_input_file,
            language="en"
        )
    os.remove(audio_input_file_path)
    return transcript


def transcribe_audio_buffer(audio_buffer):
    """
    è‡ªå‹•ä¼šè©±ãƒ¢ãƒ¼ãƒ‰ç”¨ï¼šBytesIOä¸Šã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’Whisperã§æ–‡å­—èµ·ã“ã—
    """
    audio_buffer.seek(0)
    transcript = st.session_state.openai_obj.audio.transcriptions.create(
        model="whisper-1",
        file=audio_buffer,
        language="en"
    )
    return transcript.text.strip()


# def transcribe_audio(audio_input_file_path):
#     """
#     éŸ³å£°å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
#     Args:
#         audio_input_file_path: éŸ³å£°å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
#     """

#     with open(audio_input_file_path, 'rb') as audio_input_file:
#         transcript = st.session_state.openai_obj.audio.transcriptions.create(
#             model="whisper-1",
#             file=audio_input_file,
#             language="en"
#         )

#     # éŸ³å£°å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
#     os.remove(audio_input_file_path)

#     return transcript


def save_to_wav(llm_response_audio, audio_output_file_path):
    """
    pydubã‚’ä½¿ã‚ãšã«ffmpegã‚³ãƒãƒ³ãƒ‰ã§mp3â†’wavå¤‰æ›ã™ã‚‹é–¢æ•°
    Args:
        llm_response_audio: LLMã‹ã‚‰ã®å›ç­”ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿
        audio_output_file_path: å‡ºåŠ›å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """

    # ä¸€æ™‚çš„ã«mp3ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    temp_audio_output_filename = f"{ct.AUDIO_OUTPUT_DIR}/temp_audio_output_{int(time.time())}.mp3"
    with open(temp_audio_output_filename, "wb") as temp_audio_output_file:
        temp_audio_output_file.write(llm_response_audio)

    # ffmpegã§mp3â†’wavå¤‰æ›
    subprocess.run(
        ["ffmpeg", "-y", "-i", temp_audio_output_filename, audio_output_file_path],
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL
    )

    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
    os.remove(temp_audio_output_filename)


def play_wav(audio_output_file_path, speed=1.0):
    """
    éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ä¸Šã§å†ç”Ÿï¼ˆPyAudioéä¾å­˜ç‰ˆï¼‰
    Cloudç’°å¢ƒã§ã‚‚å‹•ä½œå¯èƒ½ã€‚
    Args:
        audio_output_file_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        speed: å†ç”Ÿé€Ÿåº¦ï¼ˆæœªä½¿ç”¨ãƒ»å°†æ¥å¯¾å¿œç”¨ï¼‰
    """

    try:
        # ğŸ”¹ WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚¤ãƒŠãƒªã§èª­ã¿è¾¼ã‚€
        with open(audio_output_file_path, "rb") as f:
            audio_bytes = f.read()

        # ğŸ”¹ Streamlitã§ãƒ–ãƒ©ã‚¦ã‚¶å†ç”Ÿï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰å¯¾å¿œï¼‰
        st.audio(audio_bytes, format="audio/wav")

        # ğŸ”¹ å†ç”Ÿå¾Œã«ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆä¸è¦ãªã‚‰ã“ã®è¡Œã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
        if os.path.exists(audio_output_file_path):
            os.remove(audio_output_file_path)

    except Exception as e:
        st.error(f"éŸ³å£°ã®å†ç”Ÿä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


# def play_wav(audio_output_file_path, speed=1.0):
#     """
#     éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿ä¸Šã’
#     Args:
#         audio_output_file_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
#         speed: å†ç”Ÿé€Ÿåº¦ï¼ˆ1.0ãŒé€šå¸¸é€Ÿåº¦ã€0.5ã§åŠåˆ†ã®é€Ÿã•ã€2.0ã§å€é€Ÿãªã©ï¼‰
#     """

#     # waveãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã„ã¦å†ç”Ÿ
#     with wave.open(audio_output_file_path, 'rb') as play_target_file:
#         p = pyaudio.PyAudio()

#         # å†ç”Ÿã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’é–‹ã
#         stream = p.open(
#             format=p.get_format_from_width(play_target_file.getsampwidth()),
#             channels=play_target_file.getnchannels(),
#             rate=int(play_target_file.getframerate() * speed),
#             output=True
#         )

#         data = play_target_file.readframes(1024)
#         while data:
#             stream.write(data)
#             data = play_target_file.readframes(1024)

#         stream.stop_stream()
#         stream.close()
#         p.terminate()

#     # å†ç”Ÿå¾Œã«wavãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
#     os.remove(audio_output_file_path)


def create_chain(system_template):
    """
    LLMã«ã‚ˆã‚‹å›ç­”ç”Ÿæˆç”¨ã®Chainä½œæˆ
    """

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        MessagesPlaceholder(variable_name="history"),
        HumanMessagePromptTemplate.from_template("{input}")
    ])
    chain = ConversationChain(
        llm=st.session_state.llm,
        memory=st.session_state.memory,
        prompt=prompt
    )

    return chain


def create_problem_and_play_audio():
    """
    å•é¡Œç”Ÿæˆã¨éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†ç”Ÿ
    Args:
        chain: å•é¡Œæ–‡ç”Ÿæˆç”¨ã®Chain
        speed: å†ç”Ÿé€Ÿåº¦ï¼ˆ1.0ãŒé€šå¸¸é€Ÿåº¦ã€0.5ã§åŠåˆ†ã®é€Ÿã•ã€2.0ã§å€é€Ÿãªã©ï¼‰
        openai_obj: OpenAIã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """

    # å•é¡Œæ–‡ã‚’ç”Ÿæˆã™ã‚‹Chainã‚’å®Ÿè¡Œã—ã€å•é¡Œæ–‡ã‚’å–å¾—
    problem = st.session_state.chain_create_problem.predict(input="")

    # LLMã‹ã‚‰ã®å›ç­”ã‚’éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
    llm_response_audio = st.session_state.openai_obj.audio.speech.create(
        model="tts-1",
        voice="alloy",
        input=problem
    )

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
    audio_output_file_path = f"{ct.AUDIO_OUTPUT_DIR}/audio_output_{int(time.time())}.wav"
    save_to_wav(llm_response_audio.content, audio_output_file_path)

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿ä¸Šã’
    play_wav(audio_output_file_path, st.session_state.speed)

    return problem, llm_response_audio


def create_evaluation():
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤ã®è©•ä¾¡ç”Ÿæˆ
    """

    llm_response_evaluation = st.session_state.chain_evaluation.predict(input="")

    return llm_response_evaluation


def record_until_silence(
    timeout_sec: int = 3,
    min_silence_len_ms: int = 800,
    silence_thresh_dbfs: int = -40,
):
    """
    ğŸ¤ è‡ªå‹•è‹±ä¼šè©±ãƒ¢ãƒ¼ãƒ‰ç”¨ï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰å¯¾å¿œç‰ˆï¼‰ï¼š
    - ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã¯ streamlit-webrtc ã§è‡ªå‹•éŒ²éŸ³
    - Streamlit Cloud ãªã© webrtc_streamer ãŒä½¿ãˆãªã„ç’°å¢ƒã§ã¯ st.audio_input ã‚’ä½¿ç”¨
    æˆ»ã‚Šå€¤:
        BytesIO (wavå½¢å¼) or Noneï¼ˆéŸ³å£°ãŒå–ã‚Œãªã‹ã£ãŸå ´åˆï¼‰
    """

    # --- ğŸ” ã¾ãšã¯ webrtc ãŒä½¿ãˆã‚‹ã‹ã©ã†ã‹ç¢ºèª ---
    try:
        from streamlit_webrtc import webrtc_streamer, WebRtcMode
        webrtc_available = True
    except Exception:
        webrtc_available = False

    # --- â˜ï¸ Streamlit Cloud fallback ---
    if not webrtc_available or is_cloud:
        st.info("ğŸ¤ ä¸‹ã®ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦è©±ã—ã¦ãã ã•ã„ã€‚è©±ã—çµ‚ãˆãŸã‚‰è‡ªå‹•ã§èªè­˜ã—ã¾ã™ã€‚")

        audio = st.audio_input("ğŸ™ï¸ éŸ³å£°ã‚’éŒ²éŸ³")
        if audio is None:
            st.warning("éŒ²éŸ³ã‚’å¾…ã£ã¦ã„ã¾ã™...")
            return None

        buf = io.BytesIO(audio.read())
        buf.seek(0)
        st.success("âœ… éŸ³å£°ã‚’å–å¾—ã—ã¾ã—ãŸï¼ï¼ˆCloudãƒ¢ãƒ¼ãƒ‰ï¼‰")
        return buf

    # --- ğŸ–¥ï¸ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ¼ãƒ‰ï¼ˆwebrtcå¯¾å¿œï¼‰ ---
    st.info("ğŸ¤ è©±ã—ã¦ãã ã•ã„ã€‚è©±ã—çµ‚ãˆã¦ç´„3ç§’é»™ã‚‹ã¨ã€è‡ªå‹•ã§AIãŒè¿”ç­”ã—ã¾ã™ã€‚")

    # âœ… webrtc_streamer ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã§1å›ã ã‘åˆæœŸåŒ–
    if "webrtc_ctx" not in st.session_state:
        st.session_state.webrtc_ctx = webrtc_streamer(
            key="auto_conversation",
            mode=WebRtcMode.RECVONLY,
            media_stream_constraints={"audio": True, "video": False},
        )

    webrtc_ctx = st.session_state.webrtc_ctx

    if not webrtc_ctx.audio_receiver:
        st.warning("ãƒã‚¤ã‚¯æ¥ç¶šå¾…æ©Ÿä¸­ã§ã™...")
        return None

    audio_bytes = b""
    last_voice_time = time.time()
    started = False

    while True:
        try:
            frame = webrtc_ctx.audio_receiver.get_frame(timeout=1)
        except:
            break

        if frame is None:
            if started and (time.time() - last_voice_time) > timeout_sec:
                break
            continue

        segment = AudioSegment(
            frame.to_ndarray().tobytes(),
            sample_width=2,
            frame_rate=frame.sample_rate,
            channels=1,
        )
        audio_bytes += segment.raw_data
        started = True

        sound = AudioSegment(
            data=audio_bytes,
            sample_width=2,
            frame_rate=frame.sample_rate,
            channels=1,
        )
        nonsilent = silence.detect_nonsilent(
            sound,
            min_silence_len=min_silence_len_ms,
            silence_thresh=silence_thresh_dbfs,
        )

        if nonsilent:
            last_voice_end_ms = nonsilent[-1][1]
            last_voice_time = time.time() - (len(sound) - last_voice_end_ms) / 1000.0

        if started and (time.time() - last_voice_time) > timeout_sec:
            break

    if not audio_bytes:
        return None

    buf = io.BytesIO()
    final = AudioSegment(
        data=audio_bytes,
        sample_width=2,
        frame_rate=16000,
        channels=1,
    )
    final.export(buf, format="wav")
    buf.seek(0)

    st.success("ğŸ›‘ éŒ²éŸ³çµ‚äº†ï¼ˆè‡ªå‹•æ¤œçŸ¥ãƒ»ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ¼ãƒ‰ï¼‰")
    return buf





def generate_ai_response_auto(user_text: str):
    """
    è‡ªå‹•è‹±ä¼šè©±ãƒ¢ãƒ¼ãƒ‰ç”¨ï¼š
    - ä¼šè©±å±¥æ­´ã¤ãã§AIå¿œç­”ã‚’ç”Ÿæˆ
    - TTSã§éŸ³å£°ã‚‚ç”Ÿæˆ
    æˆ»ã‚Šå€¤:
        (ai_text: str, audio_bytes: bytes)
    """

    # ã™ã§ã« main.py å´ã§ st.session_state.llm / memory ã¯ç”¨æ„ã—ã¦ã„ã‚‹æƒ³å®š
    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(
            content=(
                "You are a friendly English conversation partner. "
                "Keep responses concise and natural. Correct the user's English gently within the reply."
            )
        ),
        MessagesPlaceholder(variable_name="history"),
        HumanMessagePromptTemplate.from_template("{input}"),
    ])

    chain = ConversationChain(
        llm=st.session_state.llm,
        memory=st.session_state.memory,
        prompt=prompt,
    )

    ai_text = chain.predict(input=user_text)

    tts_res = st.session_state.openai_obj.audio.speech.create(
        model="tts-1",
        voice="alloy",
        input=ai_text,
    )

    # OpenAI SDKã®responseã¯ .content or .read() ã§ãƒã‚¤ãƒˆåˆ—å–å¾—ï¼ˆç’°å¢ƒã«åˆã‚ã›ã¦ï¼‰
    audio_bytes = tts_res.content if hasattr(tts_res, "content") else tts_res.read()

    return ai_text, audio_bytes
